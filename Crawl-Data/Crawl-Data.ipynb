{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82a10514",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 20643/20643 [17:41<00:00, 19.45file/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary\n",
      "  Total requested: 20643\n",
      "  OK:              20336\n",
      "  Skipped:         0\n",
      "  404 not found:   307\n",
      "  Other errors:    0\n",
      "\n",
      "Some errors / non-200 responses (sample up to 50):\n",
      "  (19313, 'not_found')\n",
      "  (20001, 'not_found')\n",
      "  (20002, 'not_found')\n",
      "  (20003, 'not_found')\n",
      "  (20004, 'not_found')\n",
      "  (20005, 'not_found')\n",
      "  (20006, 'not_found')\n",
      "  (20010, 'not_found')\n",
      "  (20016, 'not_found')\n",
      "  (20011, 'not_found')\n",
      "  (20008, 'not_found')\n",
      "  (20013, 'not_found')\n",
      "  (20009, 'not_found')\n",
      "  (20012, 'not_found')\n",
      "  (20014, 'not_found')\n",
      "  (20015, 'not_found')\n",
      "  (20017, 'not_found')\n",
      "  (20018, 'not_found')\n",
      "  (20019, 'not_found')\n",
      "  (20022, 'not_found')\n",
      "  (20021, 'not_found')\n",
      "  (20020, 'not_found')\n",
      "  (20023, 'not_found')\n",
      "  (20024, 'not_found')\n",
      "  (20025, 'not_found')\n",
      "  (20026, 'not_found')\n",
      "  (20027, 'not_found')\n",
      "  (20029, 'not_found')\n",
      "  (20030, 'not_found')\n",
      "  (20032, 'not_found')\n",
      "  (20031, 'not_found')\n",
      "  (20028, 'not_found')\n",
      "  (20033, 'not_found')\n",
      "  (20034, 'not_found')\n",
      "  (20035, 'not_found')\n",
      "  (20036, 'not_found')\n",
      "  (20038, 'not_found')\n",
      "  (20037, 'not_found')\n",
      "  (20040, 'not_found')\n",
      "  (20039, 'not_found')\n",
      "  (20041, 'not_found')\n",
      "  (20043, 'not_found')\n",
      "  (20042, 'not_found')\n",
      "  (20044, 'not_found')\n",
      "  (20046, 'not_found')\n",
      "  (20048, 'not_found')\n",
      "  (20045, 'not_found')\n",
      "  (20047, 'not_found')\n",
      "  (20049, 'not_found')\n",
      "  (20051, 'not_found')\n",
      "\n",
      "Log file: download.log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "from pathlib import Path\n",
    "import time\n",
    "import logging\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from typing import Tuple\n",
    "from numpy import indices\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "from tqdm import tqdm\n",
    "\n",
    "BASE_URL = \"https://physionet.org/files/challenge-2019/1.0.0/training/training_setA\"\n",
    "FNAME_TMPL = \"p{idx:06d}.psv\"\n",
    "\n",
    "def create_session(timeout: int = 30, max_retries: int = 5, backoff_factor: float = 1.0, user_agent: str = None) -> requests.Session:\n",
    "     s = requests.Session()\n",
    "     retries = Retry(\n",
    "          total=max_retries,\n",
    "          backoff_factor=backoff_factor,\n",
    "          status_forcelist=[429, 500, 502, 503, 504],\n",
    "          allowed_methods=frozenset([\"GET\", \"HEAD\"])\n",
    "     )\n",
    "     adapter = HTTPAdapter(max_retries=retries)\n",
    "     s.mount(\"https://\", adapter)\n",
    "     s.mount(\"http://\", adapter)\n",
    "     s.headers.update({\"User-Agent\": user_agent or \"physionet-bulk-downloader/1.0\"})\n",
    "     s.request_timeout = timeout \n",
    "     return s\n",
    "\n",
    "def download_one(session: requests.Session, idx: int, out_dir: Path, timeout: int, skip_if_exists: bool, throttle: float) -> Tuple[int, str]:\n",
    "     fname = FNAME_TMPL.format(idx=idx)\n",
    "     url = f\"{BASE_URL}/{fname}\"\n",
    "     out_path = out_dir / fname\n",
    "\n",
    "     if skip_if_exists and out_path.exists():\n",
    "          return idx, \"skipped\"\n",
    "\n",
    "     tmp_path = out_path.with_suffix(\".part\")\n",
    "     try:\n",
    "          with session.get(url, stream=True, timeout=timeout) as r:\n",
    "               if r.status_code == 200:\n",
    "                    with open(tmp_path, \"wb\") as f:\n",
    "                         for chunk in r.iter_content(chunk_size=16_384):\n",
    "                              if chunk:\n",
    "                                   f.write(chunk)\n",
    "                    os.replace(tmp_path, out_path)  \n",
    "                    if throttle:\n",
    "                         time.sleep(throttle)\n",
    "                    return idx, \"ok\"\n",
    "               elif r.status_code == 404:\n",
    "                    return idx, \"not_found\"\n",
    "               else:\n",
    "                    return idx, f\"status_{r.status_code}\"\n",
    "     except Exception as e:\n",
    "          try:\n",
    "               if tmp_path.exists():\n",
    "                    tmp_path.unlink()\n",
    "          except Exception:\n",
    "               pass\n",
    "          return idx, f\"error:{e}\"\n",
    "\n",
    "def main():\n",
    "     parser = argparse.ArgumentParser(description=\"Bulk download PhysioNet .psv files (p000001.psv ...)\")\n",
    "     parser.add_argument(\"--start\", type=int, default=1, help=\"Start index (inclusive)\")\n",
    "     parser.add_argument(\"--end\", type=int, default=20643, help=\"End index (inclusive)\")\n",
    "     parser.add_argument(\"--out-dir\", type=str, default=\"./Data-Set-A\", help=\"Output directory\")\n",
    "     parser.add_argument(\"--workers\", type=int, default=8, help=\"Number of concurrent workers\")\n",
    "     parser.add_argument(\"--timeout\", type=int, default=30, help=\"Request timeout (s)\")\n",
    "     parser.add_argument(\"--retries\", type=int, default=5, help=\"Max retries per request\")\n",
    "     parser.add_argument(\"--backoff\", type=float, default=1.0, help=\"Backoff factor for retries\")\n",
    "     parser.add_argument(\"--throttle\", type=float, default=0.0, help=\"Sleep (s) after each successful download (per worker) to reduce server load\")\n",
    "     parser.add_argument(\"--skip-if-exists\", action=\"store_true\", help=\"Skip download if file already exists\")\n",
    "     parser.add_argument(\"--user-agent\", type=str, default=\"physionet-bulk-downloader/1.0\", help=\"User-Agent header\")\n",
    "     parser.add_argument(\"--make-url-list\", action=\"store_true\", help=\"Only generate a file urls.txt with all URLs (no downloading)\")\n",
    "     parser.add_argument(\"--url-list-path\", type=str, default=\"urls.txt\", help=\"Path to save url list if --make-url-list\")\n",
    "     parser.add_argument(\"--log\", type=str, default=\"download.log\", help=\"Log file\")\n",
    "     args = parser.parse_args()\n",
    "\n",
    "     logging.basicConfig(filename=args.log, level=logging.INFO, format=\"%(asctime)s %(levelname)s:%(message)s\")\n",
    "     console = logging.StreamHandler()\n",
    "     console.setLevel(logging.WARNING)\n",
    "     logging.getLogger(\"\").addHandler(console)\n",
    "\n",
    "     out_dir = Path(args.out_dir)\n",
    "     out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "     if args.make_url_list:\n",
    "          with open(args.url_list_path, \"w\", encoding=\"utf-8\") as f:\n",
    "               for i in range(args.start, args.end + 1):\n",
    "                    fname = FNAME_TMPL.format(idx=i)\n",
    "                    url = f\"{BASE_URL}/{fname}\"\n",
    "                    f.write(url + \"\\n\")\n",
    "          print(f\"Wrote URL list to {args.url_list_path}. Use `aria2c -i {args.url_list_path}` or `wget -i {args.url_list_path}` to download.\")\n",
    "          return\n",
    "\n",
    "     session = create_session(timeout=args.timeout, max_retries=args.retries, backoff_factor=args.backoff, user_agent=args.user_agent)\n",
    "\n",
    "     indices = list(range(args.start, args.end + 1))\n",
    "     results = {}\n",
    "     errors = []\n",
    "\n",
    "     with ThreadPoolExecutor(max_workers=args.workers) as ex:\n",
    "          futures = {ex.submit(download_one, session, i, out_dir, args.timeout, args.skip_if_exists, args.throttle): i for i in indices}\n",
    "          with tqdm(total=len(indices), desc=\"Downloading\", unit=\"file\") as pbar:\n",
    "               for fut in as_completed(futures):\n",
    "                    idx = futures[fut]\n",
    "                    try:\n",
    "                         idx_ret, status = fut.result()\n",
    "                    except Exception as e:\n",
    "                         idx_ret, status = idx, f\"exception:{e}\"\n",
    "                    results[idx_ret] = status\n",
    "                    if status != \"ok\" and status != \"skipped\":\n",
    "                         errors.append((idx_ret, status))\n",
    "                         logging.info(\"Index %06d -> %s\", idx_ret, status)\n",
    "                    pbar.update(1)\n",
    "\n",
    "     total = len(indices)\n",
    "     ok = sum(1 for v in results.values() if v == \"ok\")\n",
    "     skipped = sum(1 for v in results.values() if v == \"skipped\")\n",
    "     notfound = sum(1 for v in results.values() if v == \"not_found\")\n",
    "     other = total - ok - skipped - notfound\n",
    "\n",
    "     print(\"\\nSummary\")\n",
    "     print(f\"  Total requested: {total}\")\n",
    "     print(f\"  OK:              {ok}\")\n",
    "     print(f\"  Skipped:         {skipped}\")\n",
    "     print(f\"  404 not found:   {notfound}\")\n",
    "     print(f\"  Other errors:    {other}\")\n",
    "     if errors:\n",
    "          print(\"\\nSome errors / non-200 responses (sample up to 50):\")\n",
    "          for e in errors[:50]:\n",
    "               print(\" \", e)\n",
    "\n",
    "     print(f\"\\nLog file: {args.log}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "     import sys\n",
    "     sys.argv = [\"download_physionet_bulk.py\", \"--start\", \"1\", \"--end\", \"20643\", \"--skip-if-exists\"]\n",
    "     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69a11d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 20000/20000 [13:55<00:00, 23.93file/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary\n",
      "  Total requested: 20000\n",
      "  OK:              20000\n",
      "  Skipped:         0\n",
      "  404 not found:   0\n",
      "  Other errors:    0\n",
      "\n",
      "Log file: download.log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "from pathlib import Path\n",
    "import time\n",
    "import logging\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from typing import Tuple\n",
    "from numpy import indices\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "from tqdm import tqdm\n",
    "\n",
    "BASE_URL = \"https://physionet.org/files/challenge-2019/1.0.0/training/training_setB\"\n",
    "FNAME_TMPL = \"p1{idx:05d}.psv\"\n",
    "\n",
    "def create_session(timeout: int = 30, max_retries: int = 5, backoff_factor: float = 1.0, user_agent: str = None) -> requests.Session:\n",
    "     s = requests.Session()\n",
    "     retries = Retry(\n",
    "          total=max_retries,\n",
    "          backoff_factor=backoff_factor,\n",
    "          status_forcelist=[429, 500, 502, 503, 504],\n",
    "          allowed_methods=frozenset([\"GET\", \"HEAD\"])\n",
    "     )\n",
    "     adapter = HTTPAdapter(max_retries=retries)\n",
    "     s.mount(\"https://\", adapter)\n",
    "     s.mount(\"http://\", adapter)\n",
    "     s.headers.update({\"User-Agent\": user_agent or \"physionet-bulk-downloader/1.0\"})\n",
    "     s.request_timeout = timeout \n",
    "     return s\n",
    "\n",
    "def download_one(session: requests.Session, idx: int, out_dir: Path, timeout: int, skip_if_exists: bool, throttle: float) -> Tuple[int, str]:\n",
    "     fname = FNAME_TMPL.format(idx=idx)\n",
    "     url = f\"{BASE_URL}/{fname}\"\n",
    "     out_path = out_dir / fname\n",
    "\n",
    "     if skip_if_exists and out_path.exists():\n",
    "          return idx, \"skipped\"\n",
    "\n",
    "     tmp_path = out_path.with_suffix(\".part\")\n",
    "     try:\n",
    "          with session.get(url, stream=True, timeout=timeout) as r:\n",
    "               if r.status_code == 200:\n",
    "                    with open(tmp_path, \"wb\") as f:\n",
    "                         for chunk in r.iter_content(chunk_size=16_384):\n",
    "                              if chunk:\n",
    "                                   f.write(chunk)\n",
    "                    os.replace(tmp_path, out_path)  \n",
    "                    if throttle:\n",
    "                         time.sleep(throttle)\n",
    "                    return idx, \"ok\"\n",
    "               elif r.status_code == 404:\n",
    "                    return idx, \"not_found\"\n",
    "               else:\n",
    "                    return idx, f\"status_{r.status_code}\"\n",
    "     except Exception as e:\n",
    "          try:\n",
    "               if tmp_path.exists():\n",
    "                    tmp_path.unlink()\n",
    "          except Exception:\n",
    "               pass\n",
    "          return idx, f\"error:{e}\"\n",
    "\n",
    "def main():\n",
    "     parser = argparse.ArgumentParser(description=\"Bulk download PhysioNet .psv files (p000001.psv ...)\")\n",
    "     parser.add_argument(\"--start\", type=int, default=1, help=\"Start index (inclusive)\")\n",
    "     parser.add_argument(\"--end\", type=int, default=20000, help=\"End index (inclusive)\")\n",
    "     parser.add_argument(\"--out-dir\", type=str, default=\"./Data-Set-B\", help=\"Output directory\")\n",
    "     parser.add_argument(\"--workers\", type=int, default=8, help=\"Number of concurrent workers\")\n",
    "     parser.add_argument(\"--timeout\", type=int, default=30, help=\"Request timeout (s)\")\n",
    "     parser.add_argument(\"--retries\", type=int, default=5, help=\"Max retries per request\")\n",
    "     parser.add_argument(\"--backoff\", type=float, default=1.0, help=\"Backoff factor for retries\")\n",
    "     parser.add_argument(\"--throttle\", type=float, default=0.0, help=\"Sleep (s) after each successful download (per worker) to reduce server load\")\n",
    "     parser.add_argument(\"--skip-if-exists\", action=\"store_true\", help=\"Skip download if file already exists\")\n",
    "     parser.add_argument(\"--user-agent\", type=str, default=\"physionet-bulk-downloader/1.0\", help=\"User-Agent header\")\n",
    "     parser.add_argument(\"--make-url-list\", action=\"store_true\", help=\"Only generate a file urls.txt with all URLs (no downloading)\")\n",
    "     parser.add_argument(\"--url-list-path\", type=str, default=\"urls.txt\", help=\"Path to save url list if --make-url-list\")\n",
    "     parser.add_argument(\"--log\", type=str, default=\"download.log\", help=\"Log file\")\n",
    "     args = parser.parse_args()\n",
    "\n",
    "     logging.basicConfig(filename=args.log, level=logging.INFO, format=\"%(asctime)s %(levelname)s:%(message)s\")\n",
    "     console = logging.StreamHandler()\n",
    "     console.setLevel(logging.WARNING)\n",
    "     logging.getLogger(\"\").addHandler(console)\n",
    "\n",
    "     out_dir = Path(args.out_dir)\n",
    "     out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "     if args.make_url_list:\n",
    "          with open(args.url_list_path, \"w\", encoding=\"utf-8\") as f:\n",
    "               for i in range(args.start, args.end + 1):\n",
    "                    fname = FNAME_TMPL.format(idx=i)\n",
    "                    url = f\"{BASE_URL}/{fname}\"\n",
    "                    f.write(url + \"\\n\")\n",
    "          print(f\"Wrote URL list to {args.url_list_path}. Use `aria2c -i {args.url_list_path}` or `wget -i {args.url_list_path}` to download.\")\n",
    "          return\n",
    "\n",
    "     session = create_session(timeout=args.timeout, max_retries=args.retries, backoff_factor=args.backoff, user_agent=args.user_agent)\n",
    "\n",
    "     indices = list(range(args.start, args.end + 1))\n",
    "     results = {}\n",
    "     errors = []\n",
    "\n",
    "     with ThreadPoolExecutor(max_workers=args.workers) as ex:\n",
    "          futures = {ex.submit(download_one, session, i, out_dir, args.timeout, args.skip_if_exists, args.throttle): i for i in indices}\n",
    "          with tqdm(total=len(indices), desc=\"Downloading\", unit=\"file\") as pbar:\n",
    "               for fut in as_completed(futures):\n",
    "                    idx = futures[fut]\n",
    "                    try:\n",
    "                         idx_ret, status = fut.result()\n",
    "                    except Exception as e:\n",
    "                         idx_ret, status = idx, f\"exception:{e}\"\n",
    "                    results[idx_ret] = status\n",
    "                    if status != \"ok\" and status != \"skipped\":\n",
    "                         errors.append((idx_ret, status))\n",
    "                         logging.info(\"Index %06d -> %s\", idx_ret, status)\n",
    "                    pbar.update(1)\n",
    "\n",
    "     total = len(indices)\n",
    "     ok = sum(1 for v in results.values() if v == \"ok\")\n",
    "     skipped = sum(1 for v in results.values() if v == \"skipped\")\n",
    "     notfound = sum(1 for v in results.values() if v == \"not_found\")\n",
    "     other = total - ok - skipped - notfound\n",
    "\n",
    "     print(\"\\nSummary\")\n",
    "     print(f\"  Total requested: {total}\")\n",
    "     print(f\"  OK:              {ok}\")\n",
    "     print(f\"  Skipped:         {skipped}\")\n",
    "     print(f\"  404 not found:   {notfound}\")\n",
    "     print(f\"  Other errors:    {other}\")\n",
    "     if errors:\n",
    "          print(\"\\nSome errors / non-200 responses (sample up to 50):\")\n",
    "          for e in errors[:50]:\n",
    "               print(\" \", e)\n",
    "\n",
    "     print(f\"\\nLog file: {args.log}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "     import sys\n",
    "     sys.argv = [\"download_physionet_bulk.py\", \"--start\", \"1\", \"--end\", \"20000\", \"--skip-if-exists\"]\n",
    "     main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
